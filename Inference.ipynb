{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import numpy\n",
    "import albumentations\n",
    "import random\n",
    "import torch\n",
    "from albumentations.pytorch.transforms import ToTensor\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseVanillaDataset():\n",
    "    '''\n",
    "    Author @Pranav Pandey, Date: 04_03_2020.\n",
    "    This class is for loading dataset from a given folder in pairs with a label given to the pair of images;\n",
    "    if they are simillar (1) or different (0) to each other.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, imageFolderDataset, img_height, img_width, mean, std, no_template=False, transform=False):\n",
    "        self.imageFolderDataset = imageFolderDataset\n",
    "        self.no_template = no_template\n",
    "\n",
    "        if transform:\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(img_height, img_width, always_apply=True),\n",
    "                albumentations.ShiftScaleRotate(shift_limit=0.0625,\n",
    "                                scale_limit=0.1,\n",
    "                                rotate_limit=5,\n",
    "                                p=0.9),\n",
    "                albumentations.Normalize(mean, std, always_apply= True),\n",
    "                ToTensor()\n",
    "            ])\n",
    "        else:\n",
    "            self.aug = albumentations.Compose([\n",
    "                albumentations.Resize(img_height, img_width, always_apply=True),\n",
    "                albumentations.Normalize(mean, std, always_apply= True),\n",
    "                ToTensor()\n",
    "            ])\n",
    "            self.aug_2 = transforms.Compose([transforms.Resize((520,200)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                            ])\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                #keep looping till a different class image is found\n",
    "                \n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1] !=img1_tuple[1]:\n",
    "                    break\n",
    "\n",
    "        img0_raw = Image.open(img0_tuple[0]).convert(mode='RGB')\n",
    "        img0 = self.aug_2(img0_raw)\n",
    "        if(self.no_template == True):\n",
    "            img1_raw = img0_raw.filter(PIL.ImageFilter.GaussianBlur(radius=5))\n",
    "            img1 = self.aug_2(img1_raw)\n",
    "        else:\n",
    "            img1_raw = Image.open(img1_tuple[0]).convert(mode='RGB')\n",
    "            img1 = self.aug_2(img1_raw)\n",
    "        # img0 = torch.from_numpy(np.moveaxis(img0 / (255.0 if img0.dtype == np.uint8 else 1), -1, 0).astype(np.float32))\n",
    "        # img1 = torch.from_numpy(np.moveaxis(img1 / (255.0 if img1.dtype == np.uint8 else 1), -1, 0).astype(np.float32))\n",
    "        \n",
    "        return img0, img1 , torch.from_numpy(np.array([int(should_get_same_class)],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseVanilla(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseVanilla, self).__init__()\n",
    "        self.Convolve = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=32, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=2, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.Linear = nn.Sequential(\n",
    "            nn.Linear(10912,256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 24)\n",
    "        )\n",
    "    def forward(self, x_1, x_2):\n",
    "        '''\n",
    "        Keeping the passing of 2 inputs through the network explicit here for the sake of transperancy\n",
    "        '''\n",
    "        x_1 = self.Convolve(x_1)\n",
    "        x_1 = x_1.reshape(x_1.size()[0], -1)\n",
    "        x_1 = self.Linear(x_1)\n",
    "\n",
    "        x_2 = self.Convolve(x_2)\n",
    "        x_2 = x_2.reshape(x_2.size()[0], -1)\n",
    "        x_2 = self.Linear(x_2)\n",
    "        return x_1, x_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgFD_test = torchvision.datasets.ImageFolder(root=\"/home/transpacks/Repos/Siamese-Network/test/\")\n",
    "imgFD_train = torchvision.datasets.ImageFolder(root=\"/home/transpacks/Repos/Siamese-Network/input/\")\n",
    "\n",
    "test_dataset = SiameseVanillaDataset(\n",
    "    imageFolderDataset = imgFD_test,\n",
    "    img_height = 520,\n",
    "    img_width = 200,\n",
    "    mean = 0,\n",
    "    std = 0,\n",
    "    no_template=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = 1,\n",
    "    shuffle = True,\n",
    "    num_workers=4,\n",
    "    drop_last=True)\n",
    "\n",
    "train_dataset = SiameseVanillaDataset(\n",
    "    imageFolderDataset = imgFD_train,\n",
    "    img_height = 520,\n",
    "    img_width = 200,\n",
    "    mean = 0,\n",
    "    std = 0,\n",
    "    no_template=True\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = 1,\n",
    "    shuffle = True,\n",
    "    num_workers=4,\n",
    "    drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseVanilla(\n",
       "  (Convolve): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (8): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (12): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (15): ReLU()\n",
       "  )\n",
       "  (Linear): Sequential(\n",
       "    (0): Linear(in_features=10912, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=24, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SiameseVanilla()\n",
    "model.load_state_dict(torch.load(\"/home/transpacks/Repos/Siamese-Network/results/Random_Test/model_95.bin\"))\n",
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5036019062396099\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "accuracy = 0\n",
    "preds = []\n",
    "labels = []\n",
    "\n",
    "for i, (x0,x1,label) in enumerate(dataiter):\n",
    "    #concatenated = torch.cat((x0,x1),0)\n",
    "    \n",
    "    output1,output2 = model(x0.to(\"cuda:0\"),x1.to(\"cuda:0\"))\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    labels.append(label.item())\n",
    "    if (euclidean_distance <= 0.5):\n",
    "        preds.append(0)\n",
    "    else:\n",
    "        preds.append(1)\n",
    "\n",
    "print(accuracy_score(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1333 3179]\n",
      " [1300 3211]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(labels, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7145547270774841\n",
      "Label 1: 0.0 and Label 2: 0.0\n",
      "0.8847765922546387\n",
      "Label 1: 0.0 and Label 2: 1.0\n",
      "0.5884808897972107\n",
      "Label 1: 0.0 and Label 2: 1.0\n",
      "0.6108886003494263\n",
      "Label 1: 0.0 and Label 2: 1.0\n",
      "0.4660487174987793\n",
      "Label 1: 0.0 and Label 2: 1.0\n",
      "0.8675166964530945\n",
      "Label 1: 0.0 and Label 2: 0.0\n",
      "0.5574768781661987\n",
      "Label 1: 0.0 and Label 2: 0.0\n",
      "0.5961430668830872\n",
      "Label 1: 0.0 and Label 2: 1.0\n",
      "0.6219456195831299\n",
      "Label 1: 0.0 and Label 2: 0.0\n",
      "0.9052636027336121\n",
      "Label 1: 0.0 and Label 2: 0.0\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(test_loader)\n",
    "x0,_,label1 = next(dataiter)\n",
    "\n",
    "for i in range(10):\n",
    "    _,x1,label2 = next(dataiter)\n",
    "    concatenated = torch.cat((x0,x1),0)\n",
    "    \n",
    "    output1,output2 = model(x0.to(\"cuda:0\"),x1.to(\"cuda:0\"))\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    print(euclidean_distance.item())\n",
    "    print(\"Label 1: {0} and Label 2: {1}\".format(label1.item(), label2.item()))\n",
    "    #plt.imshow(torchvision.utils.make_grid(concatenated),'Similarity: {:.2f}'.format(euclidean_distance.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
